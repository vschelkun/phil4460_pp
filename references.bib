@MISC{noauthor_2022-tb,
  title        = "Faithful, Interpretable Model Explanations via Causal
                  Abstraction",
  booktitle    = "{SAIL} Blog",
  abstract     = "Seeking human-intelligible explanations",
  month        =  oct,
  year         =  2022,
  howpublished = "\url{https://ai.stanford.edu/blog/causal-abstraction/}",
  note         = "Accessed: 2024-1-23",
  keywords     = "PHIL4660 - Philosophy of Mind",
  language     = "en"
}

@ARTICLE{Olsson2022-qv,
  title         = "In-context Learning and Induction Heads",
  author        = "Olsson, Catherine and Elhage, Nelson and Nanda, Neel and
                   Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and
                   Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna
                   and Conerly, Tom and Drain, Dawn and Ganguli, Deep and
                   Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott
                   and Jones, Andy and Kernion, Jackson and Lovitt, Liane and
                   Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark,
                   Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris",
  abstract      = "``Induction heads'' are attention heads that implement a
                   simple algorithm to complete token sequences like [A][B] ...
                   [A] -> [B]. In this work, we present preliminary and
                   indirect evidence for a hypothesis that induction heads
                   might constitute the mechanism for the majority of all
                   ``in-context learning'' in large transformer models (i.e.
                   decreasing loss at increasing token indices). We find that
                   induction heads develop at precisely the same point as a
                   sudden sharp increase in in-context learning ability,
                   visible as a bump in the training loss. We present six
                   complementary lines of evidence, arguing that induction
                   heads may be the mechanistic source of general in-context
                   learning in transformer models of any size. For small
                   attention-only models, we present strong, causal evidence;
                   for larger models with MLPs, we present correlational
                   evidence.",
  month         =  sep,
  year          =  2022,
  keywords      = "PHIL4660 - Philosophy of Mind",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2209.11895"
}

@ARTICLE{Li2022-jc,
  title         = "Emergent World Representations: Exploring a Sequence Model
                   Trained on a Synthetic Task",
  author        = "Li, Kenneth and Hopkins, Aspen K and Bau, David and
                   Vi{\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg,
                   Martin",
  abstract      = "Language models show a surprising range of capabilities, but
                   the source of their apparent competence is unclear. Do these
                   networks just memorize a collection of surface statistics,
                   or do they rely on internal representations of the process
                   that generates the sequences they see? We investigate this
                   question by applying a variant of the GPT model to the task
                   of predicting legal moves in a simple board game, Othello.
                   Although the network has no a priori knowledge of the game
                   or its rules, we uncover evidence of an emergent nonlinear
                   internal representation of the board state. Interventional
                   experiments indicate this representation can be used to
                   control the output of the network and create ``latent
                   saliency maps'' that can help explain predictions in human
                   terms.",
  month         =  oct,
  year          =  2022,
  keywords      = "PHIL4660 - Philosophy of Mind",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2210.13382"
}

@INCOLLECTION{Sprevak2018-lc,
  title     = "Triviality arguments about computational implementation",
  booktitle = "The Routledge Handbook of the Computational Mind",
  author    = "Sprevak, Mark",
  publisher = "Routledge",
  pages     = "175--191",
  month     =  oct,
  year      =  2018,
  keywords  = "PHIL4660 - Philosophy of Mind",
  language  = "en"
}

@ARTICLE{noauthor_undated-fq,
  title    = "Causal Abstractions of Neural Networks",
  author   = "Geiger, Atticus and Lu, Hanson and Icard, Thomas F and Potts,
              Christopher",
  abstract = "Structural analysis methods (e.g., probing and feature
              attribution) are increasingly important tools for neural network
              analysis. We propose a new structural analysis method grounded in
              a formal theory of causal abstraction that provides rich
              characterizations of model-internal representations and their
              roles in input/output behavior. In this method, neural
              representations are aligned with variables in interpretable
              causal models, and then interchange interventions are used to
              experimentally verify that the neural representations have the
              causal properties of their aligned variables. We apply this
              method in a case study to analyze neural models trained on
              Multiply Quantified Natural Language Inference (MQNLI) corpus, a
              highly complex NLI dataset that was constructed with a
              tree-structured natural logic causal model. We discover that a
              BERT-based model with state-of-the-art performance successfully
              realizes parts of the natural logic model's causal structure,
              whereas a simpler baseline model fails to show any such
              structure, demonstrating that neural representations encode the
              compositional structure of MQNLI examples.",
  month    =  nov,
  year     =  2021,
  keywords = "PHIL4660 - Philosophy of Mind"
}

@ARTICLE{Rupert2008-ti,
  title     = "Causal theories of mental content",
  author    = "Rupert, Robert D",
  abstract  = "Causal theories of mental content (CTs) ground certain aspects
               of a concept's meaning in the causal relations a concept bears
               to what it represents. Section 1 explains the problems CTs are
               meant to solve and introduces terminology commonly used to
               discuss these problems. Section 2 specifies criteria that any
               acceptable CT must satisfy. Sections 3, 4, and 5 critically
               survey various CTs, including those proposed by Fred Dretske,
               Jerry Fodor, Ruth Garrett Millikan, David Papineau, Dennis
               Stampe, Dan Ryder, and the author himself. The final section
               considers general objections to the causal approach.",
  journal   = "Philos. Compass",
  publisher = "Wiley",
  volume    =  3,
  number    =  2,
  pages     = "353--380",
  month     =  mar,
  year      =  2008,
  keywords  = "PHIL4660 - Philosophy of Mind",
  copyright = "http://onlinelibrary.wiley.com/termsAndConditions\#vor",
  language  = "en"
}

@ARTICLE{Millikan1989-uc,
  title     = "Biosemantics",
  author    = "Millikan, Ruth Garrett",
  journal   = "J. Philos.",
  publisher = "Journal of Philosophy, Inc.",
  volume    =  86,
  number    =  6,
  pages     = "281--297",
  year      =  1989,
  keywords  = "PHIL4660 - Philosophy of Mind"
}

@ARTICLE{Green2017-ww,
  title    = "Psychosemantics and the {Rich/Thin} Debate",
  author   = "Green, E J",
  journal  = "Philos. Perspect.",
  volume   =  31,
  number   =  1,
  pages    = "153--186",
  year     =  2017,
  keywords = "PHIL4660 - Philosophy of Mind"
}

@ARTICLE{Egan2010-qg,
  title    = "Computational models: a modest role for content",
  author   = "Egan, Frances",
  abstract = "The computational theory of mind construes the mind as an
              information-processor and cognitive capacities as essentially
              representational capacities. Proponents of the view (hereafter,
              `computationalists') claim a central role for representational
              content in computational models of these capacities. In this
              paper I argue that the standard view of the role of
              representational content in computational models is mistaken; I
              argue that representational content is to be understood as a
              gloss on the computational characterization of a cognitive
              process.",
  journal  = "Stud. Hist. Philos. Sci. B Stud. Hist. Philos. Modern Phys.",
  volume   =  41,
  number   =  3,
  pages    = "253--259",
  month    =  sep,
  year     =  2010,
  keywords = "Computation; Representational content; Cognitive capacities;
              Explanation;PHIL4660 - Philosophy of Mind"
}

@ARTICLE{Hacohen2022-tq,
  title     = "The Problem with Appealing to History in Defining Neural
               Representations",
  author    = "Hacohen, Ori",
  journal   = "Eur. J. Philos. Sci.",
  publisher = "Springer Verlag",
  volume    =  12,
  number    =  3,
  pages     = "1--17",
  year      =  2022,
  keywords  = "PHIL4660 - Philosophy of Mind"
}

@ARTICLE{Milliere2024-kw,
  title         = "A Philosophical Introduction to Language Models -- Part I:
                   Continuity With Classic Debates",
  author        = "Milli{\`e}re, Rapha{\"e}l and Buckner, Cameron",
  abstract      = "Large language models like GPT-4 have achieved remarkable
                   proficiency in a broad spectrum of language-based tasks,
                   some of which are traditionally associated with hallmarks of
                   human intelligence. This has prompted ongoing disagreements
                   about the extent to which we can meaningfully ascribe any
                   kind of linguistic or cognitive competence to language
                   models. Such questions have deep philosophical roots,
                   echoing longstanding debates about the status of artificial
                   neural networks as cognitive models. This article -- the
                   first part of two companion papers -- serves both as a
                   primer on language models for philosophers, and as an
                   opinionated survey of their significance in relation to
                   classic debates in the philosophy cognitive science,
                   artificial intelligence, and linguistics. We cover topics
                   such as compositionality, language acquisition, semantic
                   competence, grounding, world models, and the transmission of
                   cultural knowledge. We argue that the success of language
                   models challenges several long-held assumptions about
                   artificial neural networks. However, we also highlight the
                   need for further empirical investigation to better
                   understand their internal mechanisms. This sets the stage
                   for the companion paper (Part II), which turns to novel
                   empirical methods for probing the inner workings of language
                   models, and new philosophical questions prompted by their
                   latest developments.",
  month         =  jan,
  year          =  2024,
  keywords      = "PHIL4660 - Philosophy of Mind",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2401.03910"
}

@UNPUBLISHED{Patel2021-cu,
  title    = "Mapping Language Models to Grounded Conceptual Spaces",
  author   = "Patel, Roma and Pavlick, Ellie",
  abstract = "A fundamental criticism of text-only language models (LMs) is
              their lack of grounding---that is, the ability to tie a word for
              which they have learned a representation, to its actual use in
              the world. However, despite this limitation, large pre-trained
              LMs have been shown to have a remarkable grasp of the conceptual
              structure of language, as demonstrated by their ability to answer
              questions, generate fluent text, or make inferences about
              entities, objects, and properties that they have never physically
              observed. In this work we investigate the extent to which the
              rich conceptual structure that LMs learn indeed reflects the
              conceptual structure of the non-linguistic world---which is
              something that LMs have never observed. We do this by testing
              whether the LMs can learn to map an entire conceptual domain
              (e.g., direction or colour) onto a grounded world representation
              given only a small number of examples. For example, we show a
              model what the word ``left`` means using a textual depiction of a
              grid world, and assess how well it can generalise to related
              concepts, for example, the word ``right'', in a similar grid
              world. We investigate a range of generative language models of
              varying sizes (including GPT-2 and GPT-3), and see that although
              the smaller models struggle to perform this mapping, the largest
              model can not only learn to ground the concepts that it is
              explicitly taught, but appears to generalise to several instances
              of unseen concepts as well. Our results suggest an alternative
              means of building grounded language models: rather than learning
              grounded representations ``from scratch'', it is possible that
              large text-only models learn a sufficiently rich conceptual
              structure that could allow them to be grounded in a
              data-efficient way.",
  month    =  oct,
  year     =  2021,
  keywords = "PHIL4660 - Philosophy of Mind"
}

@ARTICLE{Yamins2016-rq,
  title    = "Using goal-driven deep learning models to understand sensory
              cortex",
  author   = "Yamins, Daniel L K and DiCarlo, James J",
  abstract = "Fueled by innovation in the computer vision and artificial
              intelligence communities, recent developments in computational
              neuroscience have used goal-driven hierarchical convolutional
              neural networks (HCNNs) to make strides in modeling neural
              single-unit and population responses in higher visual cortical
              areas. In this Perspective, we review the recent progress in a
              broader modeling context and describe some of the key technical
              innovations that have supported it. We then outline how the
              goal-driven HCNN approach can be used to delve even more deeply
              into understanding the development and organization of sensory
              cortical processing.",
  journal  = "Nat. Neurosci.",
  volume   =  19,
  number   =  3,
  pages    = "356--365",
  month    =  mar,
  year     =  2016,
  keywords = "PHIL4660 - Philosophy of Mind",
  language = "en"
}

@ARTICLE{Chalmers1994-cg,
  title    = "On implementing a computation",
  author   = "Chalmers, David J",
  abstract = "To clarify the notion of computation and its role in cognitive
              science, we need an account of implementation, the nexus between
              abstract computations and physical systems. I provide such an
              account, based on the idea that a physical system implements a
              computation if the causal structure of the system mirrors the
              formal structure of the computation. The account is developed for
              the class of combinatorial-state automata, but is sufficiently
              general to cover all other discrete computational formalisms. The
              implementation relation is non-vacuous, so that criticisms by
              Searle and others fail. This account of computation can be
              extended to justify the foundational role of computation in
              artificial intelligence and cognitive science.",
  journal  = "Minds Mach.",
  volume   =  4,
  number   =  4,
  pages    = "391--402",
  month    =  nov,
  year     =  1994,
  keywords = "PHIL4660 - Philosophy of Mind"
}

@ARTICLE{Cao2022-dd,
  title     = "Putting Representations to Use",
  author    = "Cao, Rosa",
  journal   = "Synthese",
  publisher = "Springer Verlag",
  volume    =  200,
  number    =  2,
  year      =  2022,
  keywords  = "PHIL4660 - Philosophy of Mind"
}

@ARTICLE{Fodor1988-sa,
  title    = "Connectionism and cognitive architecture: a critical analysis",
  author   = "Fodor, J A and Pylyshyn, Z W",
  journal  = "Cognition",
  volume   =  28,
  number   = "1-2",
  pages    = "3--71",
  month    =  mar,
  year     =  1988,
  keywords = "PHIL4660 - Philosophy of Mind",
  language = "en"
}
