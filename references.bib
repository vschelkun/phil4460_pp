@ARTICLE{Li2022-jc,
  title         = "Emergent World Representations: Exploring a Sequence Model
                   Trained on a Synthetic Task",
  author        = "Li, Kenneth and Hopkins, Aspen K and Bau, David and
                   Vi{\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg,
                   Martin",
  abstract      = "Language models show a surprising range of capabilities, but
                   the source of their apparent competence is unclear. Do these
                   networks just memorize a collection of surface statistics,
                   or do they rely on internal representations of the process
                   that generates the sequences they see? We investigate this
                   question by applying a variant of the GPT model to the task
                   of predicting legal moves in a simple board game, Othello.
                   Although the network has no a priori knowledge of the game
                   or its rules, we uncover evidence of an emergent nonlinear
                   internal representation of the board state. Interventional
                   experiments indicate this representation can be used to
                   control the output of the network and create ``latent
                   saliency maps'' that can help explain predictions in human
                   terms.",
  month         =  oct,
  year          =  2022,
  keywords      = "PHIL4660 - Philosophy of Mind",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2210.13382"
}

@INCOLLECTION{Sprevak2018-lc,
  title     = "Triviality arguments about computational implementation",
  booktitle = "The Routledge Handbook of the Computational Mind",
  author    = "Sprevak, Mark",
  publisher = "Routledge",
  pages     = "175--191",
  month     =  oct,
  year      =  2018,
  keywords  = "PHIL4660 - Philosophy of Mind",
  language  = "en"
}

@UNPUBLISHED{Geiger2021-fq,
  title    = "Causal Abstractions of Neural Networks",
  author   = "Geiger, Atticus and Lu, Hanson and Icard, Thomas F and Potts,
              Christopher",
  abstract = "Structural analysis methods (e.g., probing and feature
              attribution) are increasingly important tools for neural network
              analysis. We propose a new structural analysis method grounded in
              a formal theory of causal abstraction that provides rich
              characterizations of model-internal representations and their
              roles in input/output behavior. In this method, neural
              representations are aligned with variables in interpretable
              causal models, and then interchange interventions are used to
              experimentally verify that the neural representations have the
              causal properties of their aligned variables. We apply this
              method in a case study to analyze neural models trained on
              Multiply Quantified Natural Language Inference (MQNLI) corpus, a
              highly complex NLI dataset that was constructed with a
              tree-structured natural logic causal model. We discover that a
              BERT-based model with state-of-the-art performance successfully
              realizes parts of the natural logic model's causal structure,
              whereas a simpler baseline model fails to show any such
              structure, demonstrating that neural representations encode the
              compositional structure of MQNLI examples.",
  month    =  nov,
  year     =  2021,
  keywords = "PHIL4660 - Philosophy of Mind"
}

@ARTICLE{Rupert2008-ti,
  title     = "Causal theories of mental content",
  author    = "Rupert, Robert D",
  abstract  = "Causal theories of mental content (CTs) ground certain aspects
               of a concept's meaning in the causal relations a concept bears
               to what it represents. Section 1 explains the problems CTs are
               meant to solve and introduces terminology commonly used to
               discuss these problems. Section 2 specifies criteria that any
               acceptable CT must satisfy. Sections 3, 4, and 5 critically
               survey various CTs, including those proposed by Fred Dretske,
               Jerry Fodor, Ruth Garrett Millikan, David Papineau, Dennis
               Stampe, Dan Ryder, and the author himself. The final section
               considers general objections to the causal approach.",
  journal   = "Philos. Compass",
  publisher = "Wiley",
  volume    =  3,
  number    =  2,
  pages     = "353--380",
  month     =  mar,
  year      =  2008,
  keywords  = "PHIL4660 - Philosophy of Mind",
  copyright = "http://onlinelibrary.wiley.com/termsAndConditions\#vor",
  language  = "en"
}

@ARTICLE{Green2017-ww,
  title    = "Psychosemantics and the {Rich/Thin} Debate",
  author   = "Green, E J",
  journal  = "Philos. Perspect.",
  volume   =  31,
  number   =  1,
  pages    = "153--186",
  year     =  2017,
  keywords = "PHIL4660 - Philosophy of Mind"
}

@ARTICLE{Hacohen2022-tq,
  title     = "The Problem with Appealing to History in Defining Neural
               Representations",
  author    = "Hacohen, Ori",
  journal   = "Eur. J. Philos. Sci.",
  publisher = "Springer Verlag",
  volume    =  12,
  number    =  3,
  pages     = "1--17",
  year      =  2022,
  keywords  = "PHIL4660 - Philosophy of Mind"
}

@ARTICLE{Milliere2024-kw,
  title         = "A Philosophical Introduction to Language Models -- Part I:
                   Continuity With Classic Debates",
  author        = "Milli{\`e}re, Rapha{\"e}l and Buckner, Cameron",
  abstract      = "Large language models like GPT-4 have achieved remarkable
                   proficiency in a broad spectrum of language-based tasks,
                   some of which are traditionally associated with hallmarks of
                   human intelligence. This has prompted ongoing disagreements
                   about the extent to which we can meaningfully ascribe any
                   kind of linguistic or cognitive competence to language
                   models. Such questions have deep philosophical roots,
                   echoing longstanding debates about the status of artificial
                   neural networks as cognitive models. This article -- the
                   first part of two companion papers -- serves both as a
                   primer on language models for philosophers, and as an
                   opinionated survey of their significance in relation to
                   classic debates in the philosophy cognitive science,
                   artificial intelligence, and linguistics. We cover topics
                   such as compositionality, language acquisition, semantic
                   competence, grounding, world models, and the transmission of
                   cultural knowledge. We argue that the success of language
                   models challenges several long-held assumptions about
                   artificial neural networks. However, we also highlight the
                   need for further empirical investigation to better
                   understand their internal mechanisms. This sets the stage
                   for the companion paper (Part II), which turns to novel
                   empirical methods for probing the inner workings of language
                   models, and new philosophical questions prompted by their
                   latest developments.",
  month         =  jan,
  year          =  2024,
  keywords      = "PHIL4660 - Philosophy of Mind",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2401.03910"
}

@UNPUBLISHED{Patel2021-cu,
  title    = "Mapping Language Models to Grounded Conceptual Spaces",
  author   = "Patel, Roma and Pavlick, Ellie",
  abstract = "A fundamental criticism of text-only language models (LMs) is
              their lack of grounding---that is, the ability to tie a word for
              which they have learned a representation, to its actual use in
              the world. However, despite this limitation, large pre-trained
              LMs have been shown to have a remarkable grasp of the conceptual
              structure of language, as demonstrated by their ability to answer
              questions, generate fluent text, or make inferences about
              entities, objects, and properties that they have never physically
              observed. In this work we investigate the extent to which the
              rich conceptual structure that LMs learn indeed reflects the
              conceptual structure of the non-linguistic world---which is
              something that LMs have never observed. We do this by testing
              whether the LMs can learn to map an entire conceptual domain
              (e.g., direction or colour) onto a grounded world representation
              given only a small number of examples. For example, we show a
              model what the word ``left`` means using a textual depiction of a
              grid world, and assess how well it can generalise to related
              concepts, for example, the word ``right'', in a similar grid
              world. We investigate a range of generative language models of
              varying sizes (including GPT-2 and GPT-3), and see that although
              the smaller models struggle to perform this mapping, the largest
              model can not only learn to ground the concepts that it is
              explicitly taught, but appears to generalise to several instances
              of unseen concepts as well. Our results suggest an alternative
              means of building grounded language models: rather than learning
              grounded representations ``from scratch'', it is possible that
              large text-only models learn a sufficiently rich conceptual
              structure that could allow them to be grounded in a
              data-efficient way.",
  month    =  oct,
  year     =  2021,
  keywords = "PHIL4660 - Philosophy of Mind"
}

@ARTICLE{Yamins2016-rq,
  title    = "Using goal-driven deep learning models to understand sensory
              cortex",
  author   = "Yamins, Daniel L K and DiCarlo, James J",
  abstract = "Fueled by innovation in the computer vision and artificial
              intelligence communities, recent developments in computational
              neuroscience have used goal-driven hierarchical convolutional
              neural networks (HCNNs) to make strides in modeling neural
              single-unit and population responses in higher visual cortical
              areas. In this Perspective, we review the recent progress in a
              broader modeling context and describe some of the key technical
              innovations that have supported it. We then outline how the
              goal-driven HCNN approach can be used to delve even more deeply
              into understanding the development and organization of sensory
              cortical processing.",
  journal  = "Nat. Neurosci.",
  volume   =  19,
  number   =  3,
  pages    = "356--365",
  month    =  mar,
  year     =  2016,
  keywords = "PHIL4660 - Philosophy of Mind",
  language = "en"
}

@ARTICLE{Chalmers1994-cg,
  title    = "On implementing a computation",
  author   = "Chalmers, David J",
  abstract = "To clarify the notion of computation and its role in cognitive
              science, we need an account of implementation, the nexus between
              abstract computations and physical systems. I provide such an
              account, based on the idea that a physical system implements a
              computation if the causal structure of the system mirrors the
              formal structure of the computation. The account is developed for
              the class of combinatorial-state automata, but is sufficiently
              general to cover all other discrete computational formalisms. The
              implementation relation is non-vacuous, so that criticisms by
              Searle and others fail. This account of computation can be
              extended to justify the foundational role of computation in
              artificial intelligence and cognitive science.",
  journal  = "Minds Mach.",
  volume   =  4,
  number   =  4,
  pages    = "391--402",
  month    =  nov,
  year     =  1994,
  keywords = "PHIL4660 - Philosophy of Mind"
}

@ARTICLE{Millikan1989-uc,
  title     = "Biosemantics",
  author    = "Millikan, Ruth Garrett",
  journal   = "J. Philos.",
  publisher = "Journal of Philosophy, Inc.",
  volume    =  86,
  number    =  6,
  pages     = "281--297",
  year      =  1989,
  keywords  = "PHIL4660 - Philosophy of Mind"
}

@ARTICLE{Cao2022-dd,
  title     = "Putting Representations to Use",
  author    = "Cao, Rosa",
  journal   = "Synthese",
  publisher = "Springer Verlag",
  volume    =  200,
  number    =  2,
  year      =  2022,
  keywords  = "PHIL4660 - Philosophy of Mind"
}

@ARTICLE{Kriegeskorte2019-pe,
  title    = "Peeling the Onion of Brain Representations",
  author   = "Kriegeskorte, Nikolaus and Diedrichsen, J{\"o}rn",
  abstract = "The brain's function is to enable adaptive behavior in the world.
              To this end, the brain processes information about the world. The
              concept of representation links the information processed by the
              brain back to the world and enables us to understand what the
              brain does at a functional level. The appeal of making the
              connection between brain activity and what it represents has been
              irresistible to neuroscience, despite the fact that
              representational interpretations pose several challenges: We must
              define which aspects of brain activity matter, how the code
              works, and how it supports computations that contribute to
              adaptive behavior. It has been suggested that we might drop
              representational language altogether and seek to understand the
              brain, more simply, as a dynamical system. In this review, we
              argue that the concept of representation provides a useful link
              between dynamics and computational function and ask which aspects
              of brain activity should be analyzed to achieve a
              representational understanding. We peel the onion of brain
              representations in search of the layers (the aspects of brain
              activity) that matter to computation. The article provides an
              introduction to the motivation and mathematics of
              representational models, a critical discussion of their
              assumptions and limitations, and a preview of future directions
              in this area.",
  journal  = "Annu. Rev. Neurosci.",
  volume   =  42,
  pages    = "407--432",
  month    =  jul,
  year     =  2019,
  keywords = "brain representations; decoding; encoding; neural code; pattern
              component model; representational similarity analysis;PHIL4660 -
              Philosophy of Mind",
  language = "en"
}

@ARTICLE{Egan2010-qg,
  title    = "Computational models: a modest role for content",
  author   = "Egan, Frances",
  abstract = "The computational theory of mind construes the mind as an
              information-processor and cognitive capacities as essentially
              representational capacities. Proponents of the view (hereafter,
              `computationalists') claim a central role for representational
              content in computational models of these capacities. In this
              paper I argue that the standard view of the role of
              representational content in computational models is mistaken; I
              argue that representational content is to be understood as a
              gloss on the computational characterization of a cognitive
              process.",
  journal  = "Stud. Hist. Philos. Sci. B Stud. Hist. Philos. Modern Phys.",
  volume   =  41,
  number   =  3,
  pages    = "253--259",
  month    =  sep,
  year     =  2010,
  keywords = "Computation; Representational content; Cognitive capacities;
              Explanation;PHIL4660 - Philosophy of Mind"
}

@MISC{Geiger2022-tb,
  title        = "Faithful, interpretable model explanations via causal
                  abstraction",
  booktitle    = "{SAIL} Blog",
  author       = "Geiger, Atticus and Wu, Zhengxuan and D'Oosterlinck, Karel
                  and Kreiss, Elisa and Goodman, Noah D and Icard, Thomas and
                  Potts, Christopher",
  abstract     = "Explaining why a deep learning model makes the predictions it
                  does has emerged as one of the most challenging questions in
                  AI (Lipton 2018, Pearl 2019). There is something of a paradox
                  about this, however. After all, deep learning models are
                  closed, deterministic systems that give us ground-truth
                  knowledge of the causal relationships between all their
                  components. Thus, their behavior is in many ways easy to
                  explain: one can mechanistically walk through the
                  mathematical operations or the associated computer code, and
                  this can be done at varying levels of detail. These are
                  common and valuable modes of explanation in the classroom.
                  What is missing from these explanations? They don't provide
                  human-intelligible answers to the actual questions that
                  motivate explanation methods in AI -- questions like ``Is the
                  model robust to specific kinds of input'', ``Does it treat
                  all groups fairly?'', and ``Is it safe to deploy?'' For
                  explanations that can engage with these questions, we need
                  methods that are provably faithful to the low-level details
                  (Jacovi and Goldberg 2020) but stated in higher-level
                  conceptual terms.",
  month        =  oct,
  year         =  2022,
  howpublished = "\url{https://ai.stanford.edu/blog/causal-abstraction/}",
  note         = "Accessed: 2024-1-23",
  keywords     = "PHIL4660 - Philosophy of Mind",
  language     = "en"
}

@ARTICLE{Fodor1988-sa,
  title    = "Connectionism and cognitive architecture: a critical analysis",
  author   = "Fodor, J A and Pylyshyn, Z W",
  journal  = "Cognition",
  volume   =  28,
  number   = "1-2",
  pages    = "3--71",
  month    =  mar,
  year     =  1988,
  keywords = "PHIL4660 - Philosophy of Mind",
  language = "en"
}
